{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adversarial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '/store/dataset/ChestXray-NIHCC/images'\n",
    "IMAGE_PATH = '/store/dataset/ChestXray-NIHCC/images/00017046_015.png'\n",
    "#gt = torch.FloatTensor([[0,0,0,0,0,0,0,0,0,0,0,0,1,0]]).cuda()\n",
    "from PIL import Image\n",
    "import torch\n",
    "img = Image.open(IMAGE_PATH).convert('RGB')\n",
    "\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "# transform=transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.TenCrop(224),\n",
    "#     transforms.Lambda\n",
    "#     (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "#     transforms.Lambda\n",
    "#     (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "img_torch = transform(img)\n",
    "img_torch.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "=> loaded checkpoint\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "CKPT_PATH = '/home/lrh/program/git/pytorch-example/CheXNet/model.pth.tar'\n",
    "N_CLASSES = 14\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# initialize and load the model\n",
    "model = DenseNet121(N_CLASSES).cuda()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "model.eval()\n",
    "\n",
    "if os.path.isfile(CKPT_PATH):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    checkpoint = torch.load(CKPT_PATH)\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    for key in list(state_dict.keys()):\n",
    "        res = pattern.match(key)\n",
    "        if res:\n",
    "            new_key = res.group(1) + res.group(2)\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    print(\"=> no checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2656, 0.0708, 0.8437, 0.5624, 0.6976, 0.5904, 0.4092, 0.4449, 0.3253,\n",
      "         0.0787, 0.4114, 0.8049, 0.9772, 0.2655]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[6.0785e-05, 2.0378e-06, 3.3695e-06, 3.1833e-04, 2.7117e-04, 4.3988e-04,\n",
      "         3.6961e-05, 2.9203e-04, 2.0045e-04, 3.8790e-06, 4.8334e-07, 1.1652e-06,\n",
      "         2.4777e-05, 1.1399e-05]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import libadver.attack as attack\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "pgd_params = {\n",
    "    'ord': np.inf,\n",
    "    'y': None,\n",
    "    'y_target': None,\n",
    "    'eps': 5.0 / 255,\n",
    "    'eps_iter': 0.5 / 255,\n",
    "    'nb_iter': 40,\n",
    "    'rand_init': True,\n",
    "    'rand_minmax': 5.0 / 255,\n",
    "    'clip_min': 0.,\n",
    "    'clip_max': 1.,\n",
    "    'sanity_checks': True,\n",
    "    'criterion' : nn.BCELoss()\n",
    "}\n",
    "\n",
    "pgdAttack = attack.ProjectGradientDescent(model)\n",
    "\n",
    "pgd_params[\"clip_min\"] = torch.min(img_torch)\n",
    "pgd_params[\"clip_max\"] = torch.max(img_torch)\n",
    "\n",
    "targeted = torch.FloatTensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).cuda()\n",
    "pgd_params[\"y_target\"] = targeted\n",
    "\n",
    "img = img_torch.unsqueeze(0).cuda()\n",
    "adv_x = pgdAttack.generate(img, **pgd_params)\n",
    "\n",
    "torchvision.utils.save_image(adv_x, \"adv_x.png\", normalize = True)\n",
    "torchvision.utils.save_image(img, \"img.png\", normalize = True)\n",
    "torchvision.utils.save_image(adv_x - img, \"delta_m.png\", normalize = True)\n",
    "x_pred = model(img)\n",
    "adv_pred = model(adv_x)\n",
    "\n",
    "print(x_pred)\n",
    "print(adv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "inp = img_torch.unsqueeze(0)\n",
    "#bs, n_crops, c, h, w = inp.size()\n",
    "\n",
    "input_var = inp\n",
    "#print(input_var.shape)\n",
    "output = model(input_var)\n",
    "#output_mean = output.view(bs, n_crops, -1).mean(1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8232972431555852\n",
      "0.9129112651587029\n",
      "0.8823640305627175\n",
      "0.7082440712910067\n",
      "0.8528536562120591\n",
      "0.778697148282156\n",
      "0.7738251789218398\n",
      "0.8664688403043657\n",
      "0.8086007602372302\n",
      "0.8914294840921636\n",
      "0.9100857525676305\n",
      "0.8224918776695301\n",
      "0.7774652280589532\n",
      "0.90514364827705\n",
      "The average AUROC is 0.837\n",
      "[0.8087192974635582, 0.8394775553871529, 0.8197744394418937, 0.6342887710069987, 0.8795970222440155, 0.819194936031739, 0.743502875228458, 0.7975304239290332, 0.735345250300896, 0.8071590959746802, 0.797084652075068, 0.7896848392992466, 0.7266526991485758, 0.9099986626844381]\n",
      "0.7934293228725539\n"
     ]
    }
   ],
   "source": [
    "gt_path = \"gt.npy\"\n",
    "pred_path = \"pred.npy\"\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "gt = np.load(gt_path)\n",
    "pred = np.load(pred_path)\n",
    "#print(gt[0:10])\n",
    "#print(pred[0:10])\n",
    "AUROCs = []\n",
    "for i in range(14):\n",
    "    AUROCs.append(roc_auc_score(gt[:, i], pred[:, i]))\n",
    "    print(AUROCs[i])\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "ACCs = []\n",
    "for i in range(14):\n",
    "    predictLabels = pred[:, i] > 0.5\n",
    "    #print(gt[:, i] == predictLabels)\n",
    "    acc = (gt[:, i] == predictLabels).sum() / gt.shape[0]\n",
    "    #print(acc)\n",
    "    ACCs.append(acc)\n",
    "print(ACCs)\n",
    "print(np.array(ACCs).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1907986255215102\n",
      "0.1694429281351157\n",
      "0.2555598234008782\n",
      "0.18958887080650325\n",
      "0.10203017457495783\n",
      "0.1579611181349884\n",
      "0.099630331855927\n",
      "0.14863915752179008\n",
      "0.15201406814498572\n",
      "0.22256313322909174\n",
      "0.03697507087351949\n",
      "0.13559066408234\n",
      "0.14459860340881364\n",
      "0.13130700898107445\n",
      "The average AUROC is 0.153\n",
      "[0.10631658717068604, 0.7457763116836803, 0.5614050728836981, 0.16150314269157046, 0.06089243525163821, 0.2571212053670931, 0.06454776445415236, 0.063388757633843, 0.04965898453171667, 0.5536040654393082, 0.04836624615521776, 0.3001827664601257, 0.109347835777649, 0.3017429679490037]\n",
      "0.24170386738924157\n"
     ]
    }
   ],
   "source": [
    "gt_path = \"gt.npy\"\n",
    "pred_path = \"pred_advx.npy\"\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "gt = np.load(gt_path)\n",
    "pred = np.load(pred_path)\n",
    "#print(gt[0:10])\n",
    "#print(pred[0:10])\n",
    "AUROCs = []\n",
    "for i in range(14):\n",
    "    AUROCs.append(roc_auc_score(gt[:, i], pred[:, i]))\n",
    "    print(AUROCs[i])\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "ACCs = []\n",
    "for i in range(14):\n",
    "    predictLabels = pred[:, i] > 0.5\n",
    "    #print(gt[:, i] == predictLabels)\n",
    "    acc = (gt[:, i] == predictLabels).sum() / gt.shape[0]\n",
    "    #print(acc)\n",
    "    ACCs.append(acc)\n",
    "print(ACCs)\n",
    "print(np.array(ACCs).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = \"../gt_no_tencrop.npy\"\n",
    "pred_path = \"../pred_no_tencrop.npy\"\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "gt = np.load(gt_path)\n",
    "pred = np.load(pred_path)\n",
    "#print(gt[0:10])\n",
    "#print(pred[0:10])\n",
    "AUROCs = []\n",
    "for i in range(14):\n",
    "    AUROCs.append(roc_auc_score(gt[:, i], pred[:, i]))\n",
    "    print(AUROCs[i])\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "ACCs = []\n",
    "for i in range(14):\n",
    "    predictLabels = pred[:, i] > 0.5\n",
    "    #print(gt[:, i] == predictLabels)\n",
    "    acc = (gt[:, i] == predictLabels).sum() / gt.shape[0]\n",
    "    #print(acc)\n",
    "    ACCs.append(acc)\n",
    "print(ACCs)\n",
    "print(np.array(ACCs).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, threshold=np.nan)\n",
    "#pred_path = \"../pred_no_tencrop.npy\"\n",
    "pred_path = \"pred_advx.npy\"\n",
    "pred = np.load(pred_path)\n",
    "\n",
    "predLabels = (pred>0.5).astype(np.int)\n",
    "gt = predLabels\n",
    "\n",
    "#print(gt.shape)\n",
    "labelNum = gt.shape[1]\n",
    "coMatrix = np.zeros([labelNum,labelNum])\n",
    "\n",
    "for i in range(14):\n",
    "    sourGt = gt[:,i]\n",
    "    coMatrix[i,i] = (gt[gt[:,i] == 1,:].sum(axis=1) == 1).sum()\n",
    "\n",
    "    for j in range(i+1,14):\n",
    "        desGt = gt[:,j]\n",
    "        coMatrix[i,j] = ((sourGt + desGt) == 2).sum()\n",
    "        coMatrix[j,i] = coMatrix[i,j]\n",
    "\n",
    "coMatrix = coMatrix[0:8,0:8]\n",
    "np.save(\"coMatrix_predadvx.npy\", coMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = \"../gt.npy\"\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, threshold=np.nan)\n",
    "gt = np.load(gt_path)\n",
    "\n",
    "coMatrix = np.zeros([14,14])\n",
    "\n",
    "print(coMatrix)\n",
    "\n",
    "for i in range(14):\n",
    "    sourGt = gt[:,i] \n",
    "    for j in range(i,14):\n",
    "        desGt = gt[:,j]\n",
    "        coMatrix[i,j] = ((sourGt + desGt) == 2).sum()\n",
    "print(coMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = []\n",
    "image_list_file = \"../ChestX-ray14/labels/train_list.txt\"\n",
    "with open(image_list_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        items = line.split()\n",
    "        label = items[1:]\n",
    "        label = [int(i) for i in label]\n",
    "        labels.append(label)\n",
    "gt = np.array(labels)\n",
    "\n",
    "coMatrix = np.zeros([14,14])\n",
    "\n",
    "print(coMatrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(14):\n",
    "    sourGt = gt[:,i]\n",
    "    for j in range(i,14):\n",
    "        desGt = gt[:,j]\n",
    "        coMatrix[i,j] = ((sourGt + desGt) == 2).sum()\n",
    "print(coMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = \"../ChestX-ray14/labels/train_list.txt\"\n",
    "testPath = \"../ChestX-ray14/labels/test_list.txt\"\n",
    "valPath = \"../ChestX-ray14/labels/val_list.txt\"\n",
    "labels = []\n",
    "with open(trainPath, \"r\") as f:\n",
    "    for line in f:\n",
    "        items = line.split()\n",
    "        label = items[1:]\n",
    "        label = [int(i) for i in label]\n",
    "        labels.append(label)\n",
    "with open(testPath, \"r\") as f:\n",
    "    for line in f:\n",
    "        items = line.split()\n",
    "        label = items[1:]\n",
    "        label = [int(i) for i in label]\n",
    "        labels.append(label)\n",
    "\n",
    "with open(valPath, \"r\") as f:\n",
    "    for line in f:\n",
    "        items = line.split()\n",
    "        label = items[1:]\n",
    "        label = [int(i) for i in label]\n",
    "        labels.append(label)\n",
    "\n",
    "gt = np.array(labels)\n",
    "print(gt.shape)\n",
    "\n",
    "print((gt[gt[:,0] == 1,:].sum(axis=1) == 1 ).sum())\n",
    "\n",
    "coMatrix = np.zeros([14,14])\n",
    "\n",
    "for i in range(14):\n",
    "    sourGt = gt[:,i] \n",
    "    for j in range(i,14):\n",
    "        desGt = gt[:,j]\n",
    "        coMatrix[i,j] = ((sourGt + desGt) == 2).sum()\n",
    "        coMatrix[j,i] = coMatrix[i,j]\n",
    "print(coMatrix)\n",
    "\n",
    "print(coMatrix.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute co-occurrence statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeCoccurrenceMatrix(fileList):\n",
    "    labels = []\n",
    "    for filename in fileList:\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                label = items[1:]\n",
    "                label = [int(i) for i in label]\n",
    "                labels.append(label)\n",
    "    gt = np.array(labels)\n",
    "    #print(gt.shape)\n",
    "    labelNum = gt.shape[1]\n",
    "    coMatrix = np.zeros([labelNum,labelNum])\n",
    "\n",
    "    for i in range(14):\n",
    "        sourGt = gt[:,i]\n",
    "        coMatrix[i,i] = (gt[gt[:,i] == 1,:].sum(axis=1) == 1).sum()\n",
    "        \n",
    "        for j in range(i+1,14):\n",
    "            desGt = gt[:,j]\n",
    "            coMatrix[i,j] = ((sourGt + desGt) == 2).sum()\n",
    "            coMatrix[j,i] = coMatrix[i,j]\n",
    "            \n",
    "    return coMatrix\n",
    "\n",
    "# fileList = [\"../ChestX-ray14/labels/train_list.txt\",\n",
    "#             \"../ChestX-ray14/labels/test_list.txt\",\n",
    "#             \"../ChestX-ray14/labels/val_list.txt\"]\n",
    "# file with form:\n",
    "#     [image_name, labels(0 1 1 0 0 0 0...)]\n",
    "fileList = [\"../ChestX-ray14/labels/test_list.txt\"]\n",
    "coMatrix = computeCoccurrenceMatrix(fileList)\n",
    "\n",
    "np.save(\"coMatrix_test.npy\", coMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, threshold=np.nan)\n",
    "np.load(\"coMatrix_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeCoccurrenceMatrix(fileList):\n",
    "    labels = []\n",
    "    for filename in fileList:\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                label = items[1:]\n",
    "                label = [int(i) for i in label]\n",
    "                labels.append(label)\n",
    "    gt = np.array(labels)\n",
    "    #print(gt.shape)\n",
    "    labelNum = gt.shape[1]\n",
    "    coMatrix = np.zeros([labelNum,labelNum])\n",
    "\n",
    "    for i in range(14):\n",
    "        sourGt = gt[:,i]\n",
    "        coMatrix[i,i] = (gt[gt[:,i] == 1,:].sum(axis=1) == 1).sum()\n",
    "        \n",
    "        for j in range(i+1,14):\n",
    "            desGt = gt[:,j]\n",
    "            coMatrix[i,j] = ((sourGt + desGt) == 2).sum()\n",
    "            coMatrix[j,i] = coMatrix[i,j]\n",
    "            \n",
    "    return coMatrix\n",
    "\n",
    "# fileList = [\"../ChestX-ray14/labels/train_list.txt\",\n",
    "#             \"../ChestX-ray14/labels/test_list.txt\",\n",
    "#             \"../ChestX-ray14/labels/val_list.txt\"]\n",
    "# file with form:\n",
    "#     [image_name, labels(0 1 1 0 0 0 0...)]\n",
    "fileList = [\"../ChestX-ray14/labels/test_list.txt\"]\n",
    "coMatrix = computeCoccurrenceMatrix(fileList)[0:8,0:8]\n",
    "\n",
    "np.save(\"coMatrix_test8.npy\", coMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "vgg16 = models.vgg16_bn()\n",
    "\n",
    "#print(vgg16)\n",
    "list(vgg16.features.children())[0:6]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resnet50 = models.resnet50()\n",
    "\n",
    "layer1 = resnet50.layer1\n",
    "\n",
    "#print(*list(resnet50.children())[0:4])\n",
    "\n",
    "print(resnet50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
