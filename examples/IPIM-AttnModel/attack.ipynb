{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as torch_transforms\n",
    "from networks import AttnVGG, VGG\n",
    "from loss import FocalLoss\n",
    "from data import preprocess_data_2016, preprocess_data_2017, ISIC\n",
    "from utilities import *\n",
    "from transforms import *\n",
    "\n",
    "print(\"======>load pretrained models\")\n",
    "net = AttnVGG(num_classes=2, attention=True, normalize_attn=True)\n",
    "# net = VGG(num_classes=2, gap=False)\n",
    "checkpoint = torch.load('models/checkpoint.pth')\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "pretrained_clf = nn.DataParallel(net).cuda()\n",
    "pretrained_clf.eval()\n",
    "\n",
    "print(\"=======>load ISIC2016 dataset\")\n",
    "normalize = Normalize((0.7012, 0.5517, 0.4875), (0.0942, 0.1331, 0.1521))\n",
    "transform_test = torch_transforms.Compose([\n",
    "         RatioCenterCrop(0.8),\n",
    "         Resize((256,256)),\n",
    "         CenterCrop((224,224)),\n",
    "         ToTensor(),\n",
    "         normalize\n",
    "    ])\n",
    "testset = ISIC(csv_file='test.csv', transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=False, num_workers=8)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with open('test_results.csv', 'wt', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        images_test, labels_test = data['image'], data['label']\n",
    "        images_test, labels_test = images_test.cuda(), labels_test.cuda()\n",
    "        pred_test, __, __ = pretrained_clf.forward(images_test)\n",
    "        predict = torch.argmax(pred_test, 1)\n",
    "        total += labels_test.size(0)\n",
    "        correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "        # record test predicted responses\n",
    "        responses = F.softmax(pred_test, dim=1).squeeze().detach().cpu().numpy()\n",
    "        responses = [responses[i] for i in range(responses.shape[0])]\n",
    "        csv_writer.writerows(responses)\n",
    "        # log images\n",
    "        if True:\n",
    "            I_test = utils.make_grid(images_test, nrow=8, normalize=True, scale_each=True)\n",
    "            #writer.add_image('test/image', I_test, i)\n",
    "            torchvision.utils.save_image(images_test, \"test/test_image.jpg\", nrow=8, normalize=True)\n",
    "            # accention maps\n",
    "            if True:\n",
    "                __, a1, a2 = pretrained_clf.forward(images_test)\n",
    "                if a1 is not None:\n",
    "                    attn1 = visualize_attn(I_test, a1, up_factor=8, nrow=8)\n",
    "                    torchvision.utils.save_image(attn1, \"test/att1.jpg\")\n",
    "                    #writer.add_image('test/attention_map_1', attn1, i)\n",
    "                if a2 is not None:\n",
    "                    attn2 = visualize_attn(I_test, a2, up_factor=2*8, nrow=8)\n",
    "                    torchvision.utils.save_image(attn2, \"test/att2.jpg\")\n",
    "                    #writer.add_image('test/attention_map_2', attn2, i)\n",
    "AP, AUC, precision_mean, precision_mel, recall_mean, recall_mel = compute_metrics('test_results.csv', 'test.csv')\n",
    "print(\"\\ntest result: accuracy %.2f%%\" % (100*correct/total))\n",
    "print(\"\\nmean precision %.2f%% mean recall %.2f%% \\nprecision for mel %.2f%% recall for mel %.2f%%\" %\n",
    "        (100*precision_mean, 100*recall_mean, 100*precision_mel, 100*recall_mel))\n",
    "print(\"\\nAP %.4f AUC %.4f\\n\" % (AP, AUC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libadver.attack as attack\n",
    "(./xx.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as torch_transforms\n",
    "from networks import AttnVGG, VGG\n",
    "from loss import FocalLoss\n",
    "from data import preprocess_data_2016, preprocess_data_2017, ISIC\n",
    "from utilities import *\n",
    "from transforms import *\n",
    "import libadver.attack as attack\n",
    "\n",
    "modelFile = \"/home/lrh/program/git/pytorch-example/adversarial-miccai2019/isic2016/IPMI2019-AttnMel/models/checkpoint.pth\"\n",
    "testCSVFile = \"/home/lrh/program/git/pytorch-example/adversarial-miccai2019/isic2016/IPMI2019-AttnMel/test.csv\"\n",
    "trainCSVFile = \"/home/lrh/program/git/pytorch-example/adversarial-miccai2019/isic2016/IPMI2019-AttnMel/train.csv\"\n",
    "\n",
    "print(\"======>load pretrained models\")\n",
    "net = AttnVGG(num_classes=2, attention=True, normalize_attn=True, vis = False)\n",
    "# net = VGG(num_classes=2, gap=False)\n",
    "checkpoint = torch.load(modelFile)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "pretrained_clf = nn.DataParallel(net).cuda()\n",
    "pretrained_clf.eval()\n",
    "\n",
    "print(\"=======>load ISIC2016 dataset\")\n",
    "normalize = Normalize((0.7012, 0.5517, 0.4875), (0.0942, 0.1331, 0.1521))\n",
    "transform_test = torch_transforms.Compose([\n",
    "         RatioCenterCrop(0.8),\n",
    "         Resize((256,256)),\n",
    "         CenterCrop((224,224)),\n",
    "         ToTensor(),\n",
    "         normalize\n",
    "    ])\n",
    "\n",
    "trainset = ISIC(csv_file=trainCSVFile, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "testset = ISIC(csv_file=testCSVFile, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGD attack\n",
    "1. dataset attack\n",
    "2. single image attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "pgd_params = {\n",
    "            'ord': np.inf,\n",
    "            'y': None,\n",
    "            'eps': 5.0 / 255,\n",
    "            'eps_iter': 2.5 / 255,\n",
    "            'nb_iter': 5,\n",
    "            'rand_init': True,\n",
    "            'rand_minmax': 5.0 / 255,\n",
    "            'clip_min': 0.,\n",
    "            'clip_max': 1.,\n",
    "            'sanity_checks': True\n",
    "        }\n",
    "\n",
    "import libadver.attack as attack\n",
    "PGDAttack = attack.ProjectGradientDescent(model = pretrained_clf)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "resultPath = 'adversarial_result/PGD/test_delete.csv'\n",
    "\n",
    "with open(resultPath, 'wt', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        image, label = data['image'], data['label']\n",
    "        image, label = image.cuda(), label.cuda()\n",
    "        \n",
    "        pgd_params['y'] = label\n",
    "        pgd_params['clip_min'] = torch.min(image) \n",
    "        pgd_params['clip_max'] = torch.max(image)\n",
    "        \n",
    "        adv_x = PGDAttack.generate(image, **pgd_params)\n",
    "        \n",
    "        pred_test, __, __ = pretrained_clf(adv_x)\n",
    "        predict = torch.argmax(pred_test, 1)\n",
    "        total += label.size(0)\n",
    "        correct += torch.eq(predict, label).sum().double().item()\n",
    "        # record test predicted responses\n",
    "        responses = F.softmax(pred_test, dim=1).squeeze().detach().cpu().numpy()\n",
    "        responses = [responses[i] for i in range(responses.shape[0])]\n",
    "        csv_writer.writerows(responses)\n",
    "        # log images\n",
    "#         I_test = utils.make_grid(image, nrow=8, normalize=True, scale_each=True)\n",
    "#         #writer.add_image('test/image', I_test, i)\n",
    "#         torchvision.utils.save_image(image, \"adversarial_result/PGD/test_image.jpg\", nrow=8, normalize=True)\n",
    "#         torchvision.utils.save_image(adv_x, \"adversarial_result/PGD/adv_image.jpg\", nrow=8, normalize=True)\n",
    "        \n",
    "#         # original attention maps\n",
    "#         __, a1, a2 = pretrained_clf(image)\n",
    "#         if a1 is not None:\n",
    "#             attn1 = visualize_attn(I_test, a1, up_factor=8, nrow=8)\n",
    "#             torchvision.utils.save_image(attn1, \"adversarial_result/PGD/att1.jpg\")\n",
    "#             #writer.add_image('test/attention_map_1', attn1, i)\n",
    "#         if a2 is not None:\n",
    "#             attn2 = visualize_attn(I_test, a2, up_factor=2*8, nrow=8)\n",
    "#             torchvision.utils.save_image(attn2, \"adversarial_result/PGD/att2.jpg\")\n",
    "            \n",
    "#         # adversarial attention maps\n",
    "#         __, a1, a2 = pretrained_clf(adv_x)\n",
    "#         if a1 is not None:\n",
    "#             attn1 = visualize_attn(I_test, a1, up_factor=8, nrow=8)\n",
    "#             torchvision.utils.save_image(attn1, \"adversarial_result/PGD/adv_att1.jpg\")\n",
    "#             #writer.add_image('test/attention_map_1', attn1, i)\n",
    "#         if a2 is not None:\n",
    "#             attn2 = visualize_attn(I_test, a2, up_factor=2*8, nrow=8)\n",
    "#             torchvision.utils.save_image(attn2, \"adversarial_result/PGD/adv_att2.jpg\")\n",
    "#         break\n",
    "                    #writer.add_image('test/attention_map_2', attn2, i)\n",
    "# AP, AUC, precision_mean, precision_mel, recall_mean, recall_mel = compute_metrics(resultPath, 'test.csv')\n",
    "# print(\"\\ntest result: accuracy %.2f%%\" % (100*correct/total))\n",
    "# print(\"\\nmean precision %.2f%% mean recall %.2f%% \\nprecision for mel %.2f%% recall for mel %.2f%%\" %\n",
    "#         (100*precision_mean, 100*recall_mean, 100*precision_mel, 100*recall_mel))\n",
    "# print(\"\\nAP %.4f AUC %.4f\\n\" % (AP, AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *\n",
    "resultPath = \"adversarial_result/PGD/test_m5n5.csv\"\n",
    "testCSVFile = \"/home/lrh/program/git/pytorch-example/adversarial-miccai2019/isic2016/IPMI2019-AttnMel/test.csv\"\n",
    "AP, AUC, precision_mean, precision_mel, recall_mean, recall_mel = compute_metrics(resultPath, testCSVFile)\n",
    "#print(\"\\ntest result: accuracy %.2f%%\" % (100*correct/total))\n",
    "print(\"\\nmean precision %.2f%% mean recall %.2f%% \\nprecision for mel %.2f%% recall for mel %.2f%%\" %\n",
    "        (100*precision_mean, 100*recall_mean, 100*precision_mel, 100*recall_mel))\n",
    "print(\"\\nAP %.4f AUC %.4f\\n\" % (AP, AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "from transforms import *\n",
    "import libadver\n",
    "\n",
    "pgd_params = {\n",
    "            'ord': np.inf,\n",
    "            'y': None,\n",
    "            'eps': 5.0 / 255,\n",
    "            'eps_iter': 1.25 / 255,\n",
    "            'nb_iter': 10,\n",
    "            'rand_init': True,\n",
    "            'rand_minmax': 5.0 / 255,\n",
    "            'clip_min': 0.,\n",
    "            'clip_max': 1.,\n",
    "            'sanity_checks': True\n",
    "        }\n",
    "\n",
    "pgd_params['y'] = torch.LongTensor([1,1,1,1,1]).cuda()\n",
    "\n",
    "\n",
    "import libadver.attack as attack\n",
    "PGDAttack = attack.ProjectGradientDescent(model = pretrained_clf)\n",
    "isBenign = False\n",
    "benignRoot = \"./adversarial_result/ori_img/benign\"\n",
    "malignantRoot = \"./adversarial_result/ori_img/malignant\"\n",
    "\n",
    "benignImgs = [\n",
    "    \"ISIC_0000234.jpg\",\"ISIC_0000254.jpg\", \"ISIC_0000271.jpg\", \n",
    "    \"ISIC_0000325.jpg\",\"ISIC_0000319.jpg\"\n",
    "]\n",
    "malignImgs = [\n",
    "    \"ISIC_0000549.jpg\", \"ISIC_0001103.jpg\", \"ISIC_0001142.jpg\",\n",
    "    \"ISIC_0000547.jpg\", \"ISIC_0001100.jpg\"\n",
    "]\n",
    "\n",
    "if isBenign is True:\n",
    "    Imgs = benignImgs\n",
    "    Root = benignRoot\n",
    "else:\n",
    "    Imgs = malignImgs\n",
    "    Root = malignantRoot\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "mean = [0.7012, 0.5517, 0.4875]\n",
    "std = [0.0942, 0.1331, 0.1521]\n",
    "normalize = Normalize(mean, std)\n",
    "transform_test = transforms.Compose([\n",
    "         RatioCenterCrop(0.8),\n",
    "         Resize((256,256)),\n",
    "         CenterCrop((224,224)),\n",
    "         ToTensor(),\n",
    "         normalize\n",
    "    ])\n",
    "\n",
    "images = torch.zeros([5,3,224,224])\n",
    "labels = torch.zeros([5])\n",
    "from PIL import Image\n",
    "for batchIdx, Img in enumerate(Imgs):\n",
    "    benignPath = os.path.join(Root, Img)\n",
    "    img = Image.open(benignPath)\n",
    "    sample = {'image': img, 'image_seg': img, 'label': 0}\n",
    "    t_sample = transform_test(sample)\n",
    "    img = t_sample[\"image\"]\n",
    "    #img.unsqueeze_(0)\n",
    "    images[batchIdx] = img\n",
    "    labels[batchIdx] = t_sample['label']\n",
    "    #print(images.shape)\n",
    "# print(img1.shape)\n",
    "images = images.cuda()\n",
    "pgd_params['clip_min'] = torch.min(images) \n",
    "pgd_params['clip_max'] = torch.max(images)\n",
    "\n",
    "# img1_temp = torch.zeros(img1.size())\n",
    "# for c2 in range(3):\n",
    "#     img1_temp.data[:,c2,:,:] = (img1.data[:,c2,:,:] * std[c2]) + mean[c2]\n",
    "# torchvision.utils.save_image(img1_temp, \"test/ori_img.jpg\", normalize=True,  scale_each=True)\n",
    "\n",
    "\n",
    "adv_images = PGDAttack.generate(images, **pgd_params)\n",
    "\n",
    "#torchvision.utils.save_image(images, \"adversarial_result/PGD/ori_img.png\", normalize=True,  scale_each=True)\n",
    "#torchvision.utils.save_image(adv_images, \"adversarial_result/PGD/adv_img.png\", normalize=True,  scale_each=True)\n",
    "delta_ims = adv_images - images\n",
    "#torchvision.utils.save_image(delta_ims, \"adversarial_result/PGD/delta_im.png\", normalize=True,  scale_each=True)\n",
    "\n",
    "\n",
    "pred_test, a1, a2 = pretrained_clf(images)\n",
    "print(torch.softmax(pred_test, dim=1))\n",
    "predict = torch.argmax(pred_test, 1)\n",
    "print(predict)\n",
    "\n",
    "pred_test, a1, a2 = pretrained_clf(adv_images)\n",
    "print(torch.softmax(pred_test, dim=1))\n",
    "predict = torch.argmax(pred_test, 1)\n",
    "print(predict)\n",
    "\n",
    "images = images.cpu()\n",
    "adv_images = adv_images.cpu()\n",
    "delta_ims = delta_ims.cpu()\n",
    "\n",
    "for i in range(5):\n",
    "    image = images[i]\n",
    "    image_PIL = libadver.visutils.recreate_image(image,mean,std)\n",
    "    libadver.visutils.save_image(image_PIL,\"adversarial_result/pre_img/malignant/ori_img_%d.png\" %i)\n",
    "    \n",
    "    adv_image = adv_images[i]\n",
    "    adv_image_PIL = libadver.visutils.recreate_image(adv_image,mean,std)\n",
    "    libadver.visutils.save_image(adv_image_PIL,\"adversarial_result/PGD/malignant/adv_img_%d.png\" %i)\n",
    "    \n",
    "    delta_im = delta_ims[i].data.numpy()\n",
    "    libadver.visutils.save_gradient_images(delta_im,\"adversarial_result/PGD/malignant/delta_im_%d.png\" %i)\n",
    "    \n",
    "    \n",
    "\n",
    "#I_test = utils.make_grid(img1.detach(), nrow=8, normalize=True, scale_each=True)\n",
    "\n",
    "# for c2 in range(3):\n",
    "#     img1_temp.data[:,c2,:,:] = (img1.data[:,c2,:,:] * std[c2]) + mean[c2]\n",
    "# torchvision.utils.save_image(img1_temp, \"test/adv_img.jpg\", normalize=True,  scale_each=True)\n",
    "\n",
    "# if a1 is not None:\n",
    "#     attn1 = visualize_attn(I_test, a1, up_factor=8, nrow=8)\n",
    "#     torchvision.utils.save_image(attn1, \"test/adv_att1.jpg\")\n",
    "#     #writer.add_image('test/attention_map_1', attn1, i)\n",
    "# if a2 is not None:\n",
    "#     attn2 = visualize_attn(I_test, a2, up_factor=2*8, nrow=8)\n",
    "#     torchvision.utils.save_image(attn2, \"test/adv_att2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test draw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from libadver import visutils\n",
    "\n",
    "malignantRoot = \"./adversarial_result/pre_img/malignant\"\n",
    "malignImgs = [\n",
    "    \"ori_img_0.png\", \"ori_img_1.png\", \"ori_img_2.png\",\n",
    "    \"ori_img_3.png\", \"ori_img_4.png\"\n",
    "]\n",
    "malignantAdvRoot = \"./adversarial_result/PGD/malignant\"\n",
    "malignAdvImgs = [\n",
    "    \"adv_img_0.png\", \"adv_img_1.png\", \"adv_img_2.png\",\n",
    "    \"adv_img_3.png\", \"adv_img_4.png\"\n",
    "]\n",
    "\n",
    "\n",
    "# malignImg = malignImgs[0]\n",
    "# malignPath = os.path.join(malignantRoot, malignImg)\n",
    "# img = Image.open(malignPath)\n",
    "# text = \"Malignant:98.93%\"\n",
    "# img_text = visutils.draw_text(img,text,'green',length=180)\n",
    "# img_text.save(os.path.join(malignantRoot, \"ori_img_0_text.png\"))\n",
    "\n",
    "malignAdvImg = malignAdvImgs[0]\n",
    "malignAdvPath = os.path.join(malignantAdvRoot, malignAdvImg)\n",
    "advImg = Image.open(malignAdvPath)\n",
    "text = \"Benign:100%\"\n",
    "advImg_text = visutils.draw_text(advImg,text,'red',length=180)\n",
    "advImg_text.save(os.path.join(malignantAdvRoot, \"adv_img_0_text.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAP attack\n",
    "1. dataset attack\n",
    "2. single image attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libadver.attack as attack\n",
    "import libadver.models.generators as generators\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "mean = (0.7012, 0.5517, 0.4875)\n",
    "std = (0.0942, 0.1331, 0.1521)\n",
    "isTrain = True\n",
    "import torch\n",
    "\n",
    "params = {\n",
    "        \"attackModelPath\" : None,\n",
    "        \"mag_in\" : 7.0,\n",
    "        \"ord\" : \"inf\",\n",
    "        \"epochNum\" : 3,\n",
    "        \"criterion\" : nn.CrossEntropyLoss(),\n",
    "        \"ncInput\" : 3,\n",
    "        \"ncOutput\" : 3,\n",
    "        \"mean\" : mean,\n",
    "        \"std\" : std,\n",
    "        \"MaxIter\" : 100\n",
    "    }\n",
    "print(params)\n",
    "saveModelPath = \"adversarial_result/GAP/GAP_im_m7n3.pth\"\n",
    "attackModel = generators.define(input_nc = params[\"ncInput\"], output_nc = params[\"ncOutput\"],\n",
    "                                ngf = 64, gen_type = \"unet\", norm=\"batch\", act=\"relu\", gpu_ids = [0])\n",
    "    \n",
    "\n",
    "if isTrain is True:\n",
    "    print(\"===>Train\")\n",
    "    optimizerG = optim.Adam(attackModel.parameters(), lr = 2e-4, betas = (0.5, 0.999))\n",
    "    params[\"optimizerG\"] = optimizerG\n",
    "    GAPAttack = attack.GenerativeAdversarialPerturbations(pretrained_clf, attackModel, **params)\n",
    "    GAPAttack.train(trainloader, saveModelPath)\n",
    "else:\n",
    "    print(\"===>Test\")\n",
    "    ## test\n",
    "    params[\"attackModelPath\"] = saveModelPath\n",
    "    GAPAttack = attack.GenerativeAdversarialPerturbations(pretrained_clf, attackModel, **params)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(testloader):\n",
    "        images, targets = data['image'].cuda(), data['label'].cuda()\n",
    "        adv_images = GAPAttack.generate(images)\n",
    "        predicted,_,_ = pretrained_clf(adv_images)\n",
    "        predicted_labels = torch.argmax(predicted,1)\n",
    "        #print(predicted_labels)\n",
    "        correct += torch.sum(predicted_labels.eq(targets))\n",
    "        #print(targets)\n",
    "        total += images.shape[0]\n",
    "        print(\"ACC:%.3f | %d,%d\" %(100.0*float(correct) / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visulization\n",
    "\n",
    "ori_img + conv1 feas + saliency maps + attention maps + Guided gradCam\n",
    "\n",
    "//vistools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libadver.visutils as visutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transforms import *\n",
    "from torchvision.transforms import transforms\n",
    "benignImg = \"./test/ori_img/ISIC_0000016_be.jpg\" #0\n",
    "malignImg = \"./test/ori_img/ISIC_0000074_ma.jpg\" #1\n",
    "target_class = 1\n",
    "img1 = Image.open(malignImg)\n",
    "\n",
    "mean = (0.7012, 0.5517, 0.4875)\n",
    "std = (0.0942, 0.1331, 0.1521)\n",
    "normalize = Normalize(mean, std)\n",
    "transform_test = transforms.Compose([\n",
    "         RatioCenterCrop(0.8),\n",
    "         Resize((256,256)),\n",
    "         CenterCrop((224,224)),\n",
    "         ToTensor(),\n",
    "         normalize\n",
    "    ])\n",
    "\n",
    "sample = {'image': img1, 'image_seg': img1, 'label': 0}\n",
    "\n",
    "t_sample = transform_test(sample)\n",
    "img1 = t_sample[\"image\"]\n",
    "img1.unsqueeze_(0)\n",
    "img1 = img1.cuda()\n",
    "\n",
    "x = img1\n",
    "x.requires_grad = True\n",
    "output,_,_ = pretrained_clf(x)\n",
    "\n",
    "\n",
    "one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_()\n",
    "one_hot_output[0][target_class] = 1\n",
    "one_hot_output = one_hot_output.cuda()\n",
    "# Backward pass\n",
    "output.backward(gradient=one_hot_output)\n",
    "print(x.grad.data.shape)\n",
    "gradient_arr = x.grad.data.cpu().numpy()[0]\n",
    "gradient_arr_grey = visutils.convert_to_grayscale(gradient_arr)\n",
    "print(gradient_arr_grey.shape)\n",
    "gradient_arr_grey = 1 - gradient_arr_grey\n",
    "visutils.save_gradient_images(gradient_arr_grey, \"test/malignImg_grey.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transforms import *\n",
    "from torchvision.transforms import transforms\n",
    "benignImg = \"./test/ori_img/ISIC_0000016_be.jpg\" #0\n",
    "malignImg = \"./test/ori_img/ISIC_0000074_ma.jpg\" #1\n",
    "target_class = 1\n",
    "img1 = Image.open(malignImg)\n",
    "\n",
    "mean = (0.7012, 0.5517, 0.4875)\n",
    "std = (0.0942, 0.1331, 0.1521)\n",
    "normalize = Normalize(mean, std)\n",
    "transform_test = transforms.Compose([\n",
    "         RatioCenterCrop(0.8),\n",
    "         Resize((256,256)),\n",
    "         CenterCrop((224,224)),\n",
    "         ToTensor(),\n",
    "         normalize\n",
    "    ])\n",
    "\n",
    "sample = {'image': img1, 'image_seg': img1, 'label': 0}\n",
    "\n",
    "t_sample = transform_test(sample)\n",
    "img1 = t_sample[\"image\"]\n",
    "img1.unsqueeze_(0)\n",
    "img1 = img1.cuda()\n",
    "\n",
    "x = img1\n",
    "_,_,_,block1,block2,block3,block4,block5 = pretrained_clf(x)\n",
    "\n",
    "print(block1.shape)\n",
    "import torchvision\n",
    "block1 = block1.permute([1,0,2,3])\n",
    "torchvision.utils.save_image(block1,\"test/block1.jpg\")\n",
    "\n",
    "block2 = block2.permute([1,0,2,3])\n",
    "torchvision.utils.save_image(block2,\"test/block2.jpg\")\n",
    "\n",
    "block3 = block3.permute([1,0,2,3])\n",
    "torchvision.utils.save_image(block3,\"test/block3.jpg\",nrow=32)\n",
    "\n",
    "block4 = block4.permute([1,0,2,3])\n",
    "torchvision.utils.save_image(block4,\"test/block4.jpg\",nrow=32)\n",
    "\n",
    "block5 = block5.permute([1,0,2,3])\n",
    "torchvision.utils.save_image(block5,\"test/block5.jpg\",nrow=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "crashes = sns.load_dataset(\"car_crashes\").sort_values(\"total\", ascending=False)[0:7]\n",
    "print(crashes.head())\n",
    "# 加载数据\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6, 3))\n",
    "# 创建图表\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"total\", y=\"abbrev\", data=crashes,\n",
    "            label=\"Total\", color=\"b\",edgecolor = 'w')\n",
    "# 设置第一个柱状图\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=\"alcohol\", y=\"abbrev\", data=crashes,\n",
    "            label=\"Alcohol-involved\", color=\"b\",edgecolor = 'w')\n",
    "# 设置第二个柱状图\n",
    "\n",
    "ax.legend(ncol=1, loc=\"lower right\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnext = models.resnext101_32x8d(pretrained=True)\n",
    "\n",
    "#print(resnext)\n",
    "\n",
    "resnet = models.resnet18()\n",
    "#print(resnet)\n",
    "\n",
    "print(resnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32.0844 27.3352 26.0158 25.1186]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "acc_np = np.array([\n",
    "    [27.968,34.301,36.939,30.343,30.871],\n",
    "    [27.177,27.177,26.385,25.330,30.607],\n",
    "    [28.760,22.691,25.330,22.691,30.607],\n",
    "    [22.955,27.704,23.219,27.441,24.274]\n",
    "])\n",
    "\n",
    "import pandas\n",
    "data_df = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "print(acc_np.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f675a517160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "fmri = sns.load_dataset(\"fmri\")\n",
    "\n",
    "#print(fmri)\n",
    "\n",
    "ax = sns.lineplot(x=\"lambda\", y=\"acc\", markers=True, dashes=False, data=data_df)\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
